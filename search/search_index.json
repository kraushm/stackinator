{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stackinator","text":"<p>A tool for building a scientific software stack from a recipe on HPE Cray EX systems.</p> <p>It is used to build software vClusters on Alps infrastructure at CSCS.</p>"},{"location":"#getting-stackinator","title":"Getting Stackinator","text":""},{"location":"#from-github-recommended","title":"From GitHub (recommended)","text":"<p>To get the latest version, download directly from GitHub.</p> <pre><code>git clone https://github.com/eth-cscs/stackinator.git\ncd stackinator\n./bootstrap.sh\n</code></pre> <p>The <code>bootstrap.sh</code> script will install the necessary dependencies, so that Stackinator can be run as a standalone application.</p> <p>Once installed, add the <code>bin</code> sub-directory to your path:</p> <pre><code>export PATH=\"&lt;stackinator-install-path&gt;/bin:$PATH\"\n</code></pre>"},{"location":"#using-pip","title":"Using Pip","text":"<p>Stackinator is available on PyPi:</p> <pre><code>pip install stackinator\n</code></pre> <p>Warning</p> <p>The PyPi package is only updated for releases, so you will likely be missing the latest and greatest features. Let us know if you need more regular PyPi updates.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Stackinator generates the make files and spack configurations that build the spack environments that are packaged together in the spack stack. It can be thought of as equivalent to calling <code>cmake</code> or <code>configure</code>, before running make to run the configured build.</p> <pre><code># configure the build\nstack-config --build $BUILD_PATH --recipe $RECIPE_PATH --system $SYSTEM_CONFIG_PATH\n</code></pre> <p>Where the <code>BUILD_PATH</code> is the path where the build will be configured, the <code>RECIPE_PATH</code> contains the recipe for the sotware stack, and <code>SYSTEM_CONFIG_PATH</code> is the system configuration for the cluster being targeted.</p> <p>Once configured, the build stack is built in the build path using make:</p> <pre><code># build the spack stack\ncd $BUILD_PATH\nenv --ignore-environment PATH=/usr/bin:/bin:`pwd`/spack/bin make modules store.squashfs -j64\n</code></pre> <p>See the documentation on building Spack stacks for more information.</p> <p>Once the build has finished successfully the software can be installed.</p> <p>Alps</p> <p>On Alps the software stack can be tested using the SquashFS image generated by the build: <pre><code>squashfs-mount store.squashfs /user-environment bash\nls /user-environment\n</code></pre></p>"},{"location":"build-caches/","title":"Build Caches","text":"<p>Stackinator facilitates using Spack's binary build caches to speed up image builds. Build caches are essential if you plan to build images regularly, as they generally lead to a roughly 10x speed up. This is the difference between half an hour or 3 minutes to build a typical image.</p>"},{"location":"build-caches/#using-build-caches","title":"Using Build caches","text":"<p>To use a build cache, create a simple YAML file:</p> cache-config.yaml<pre><code>root: $SCRATCH/uenv-cache\nkey:  $SCRATCH/.keys/spack-push-key.gpg\n</code></pre> <p>To use the cache, pass the configuration as an option to <code>stack-config</code> via the <code>-c/--cache</code> flag:</p> <pre><code>stack-config -b $build_path -r $recipe_path -s $system_config -c cache-config.yaml\n</code></pre> If you using an old binary build cache <p>Since v3, Stackinator creates a sub-directory in the build cache for each mount point. For example, in the above example, the build cache for the <code>/user-environment</code> mount point would be <code>$SCRATCH/uenv-cache/user-environment</code>. The rationale for this is so that packages for different mount points are not mixed, to avoid having to relocate binaries.</p> <p>To continue using a build caches from before v3, first copy the <code>build_cache</code> path to a subdirectory, e.g.:</p> <pre><code>mkdir $SCRATCH/uenv-cache/user-environment\nmv $SCRATCH/uenv-cache/build_cache $SCRATCH/uenv-cache/user-environment\n</code></pre>"},{"location":"build-caches/#build-only-caches","title":"Build-only caches","text":"<p>A build cache can be configured to be read-only by not providing a <code>key</code> in the cache configuration file.</p>"},{"location":"build-caches/#creating-a-build-cache","title":"Creating a Build Cache","text":"<p>To create a build cache we need two things:</p> <ol> <li>An empty directory where the cache will be populated by Spack.</li> <li>A private PGP key<ul> <li>Only required for Stackinator to push packages to the cache when it builds a package that was not in the cache.</li> </ul> </li> </ol> <p>Creating the cache directory is easy! For example, to create a cache on your scratch storage: <pre><code>mkdir $SCRATCH/uenv-cache\n</code></pre></p>"},{"location":"build-caches/#generating-keys","title":"Generating Keys","text":"<p>An installation of Spack can be used to generate the key file:</p> <pre><code># create a key\nspack gpg create &lt;name&gt; &lt;e-mail&gt;\n\n# export key\nspack gpg export --secret spack-push-key.gpg\n</code></pre> <p>See the spack documentation for more information about GPG keys.</p>"},{"location":"build-caches/#managing-keys","title":"Managing Keys","text":"<p>The key needs to be in a location that is accessible during the build process, and secure. To keep your PGP key secret, you can generate it then move it to a path with appropriate permissions. In the example below, we create a path <code>.keys</code> for storing the key: <pre><code># create  .keys path is visible only to you\nmkdir $SCRATCH/.keys\nchmod 700 $SCRATCH/.keys\n\n# generate the key\nspack gpg create &lt;name&gt; &lt;e-mail&gt;\nspack gpg export --secret $SCRATCH/.keys/spack-push-key.gpg\nchmod 600 $SCRATCH/.keys/spack-push-key.gpg\n</code></pre></p> <p>The cache-configuration would look like the following, where we assume that the cache is in <code>$SCRATCH/uenv-cache</code>: <pre><code>root: $SCRATCH/uenv-cache\nkey: $SCRATCH/.keys/spack-push-key.gpg\n</code></pre></p> <p>Warning</p> <p>Don't blindly copy this documentation's advice on security settings.</p> <p>Don't use <code>$HOME</code></p> <p>Don't put the keys in <code>$HOME</code>, because the build process remounts <code>~</code> as a tmpfs, and you will get error messages that Spack can't read the key.</p>"},{"location":"building/","title":"Building Spack Stacks","text":"<p>Once a stack has been configured using <code>stack-config</code>, it's time to build the software stack.</p>"},{"location":"building/#how-to-build","title":"How to Build","text":"<p>The configuration generates a build path, with a top-level <code>Makefile</code> that performs the build.</p> <pre><code># configure the build\nstack-config --build $BUILD_PATH ...\n\n# perform the build\ncd $BUILD_PATH\nenv --ignore-environment PATH=/usr/bin:/bin:`pwd`/spack/bin make modules store.squashfs -j32\n</code></pre> <p>The call to <code>make</code> is wrapped with with <code>env --ignore-env</code> to unset all environment variables, to improve reproducability of builds.</p> <p>Build times for stacks typically vary between 30 minutes to 3 hours, depending on the specific packages that have to be built. Using build caches and building in shared memory (see below) are the most effective methods to speed up builds.</p>"},{"location":"building/#where-to-build","title":"Where to Build","text":"<p>Spack detects the CPU \u03bc-arch that it is being run on, and configures the packages to target it. In order to ensure the best results, build the stack on a compute node with the target architecture, not a login node.</p> <p>Alps</p> <p>Alps vClusters often have different CPU \u03bc-arch on login nodes (zen2) and compute nodes (zen3).</p> <p>Build times can be signficantly reduced by creating the build path in memory, for example in <code>/dev/shm/$USER/build</code>, so that all of the dependencies are built and stored in memory, instead of on a slower shared file system.</p> <p>Alps</p> <p>All of the Cray EX nodes on Alps have 512 GB of memory, which is sufficient for building software stacks, though it is important that the memory is cleaned up, preferably via an automated policy.</p> <p>Warning</p> <p>Take care to remove the build path when building in shared memory -- otherwise it will reduce the amount of memory available for later users of the node, because some clusters do not automatically clean up <code>/dev/shm</code> on compute nodes -- and <code>/dev/shm</code> is only cleared on login nodes when they are reset.</p>"},{"location":"cluster-config/","title":"Cluster Configuration","text":"<p>Spack stacks are built on bare-metal clusters using a minimum of dependencies from the underlying system. A cluster configuration is a directory with the following structure:</p> <pre><code>/path/to/cluster/configuration\n\u251c\u2500 compilers.yaml   # system compiler\n\u251c\u2500 packages.yaml    # external system packages\n\u2514\u2500 concretiser.yaml\n</code></pre> <p>The configuration is provided during the configuration step with the <code>--system/-s</code> flag. The following example targets the Clariden system at CSCS:</p> <pre><code>git clone git@github.com:eth-cscs/alps-cluster-config.git\nstack-config --system ./alps-cluster-config/clariden --recipe &lt;recipe path&gt; --build &lt;build path&gt;\n</code></pre> <p>Alps</p> <p>The CSCS official configuration for vClusters on Alps are maintained in a GitHub repository github.com/eth-cscs/alps-cluster-config.</p> <p>Software stacks provided by CSCS will only use the official configuration, and support will only be provided for user-built stacks that used the official configuration.</p> <p>If there are additional system packages that you want to use in a recipe, consider adding a <code>packages.yaml</code> file to the recipe, in which you can define additional external packages.</p> <p>Warning</p> <p>Only use external dependencies that are strictly necessary:</p> <ul> <li>the more dependencies, the more potential that software stacks will have to be rebuilt when the system is updated, and the more potential there are for breaking changes;</li> <li>the external packages are part of the Spack upstream configuration generated with the Stack - you might be constraining the choices of downstream users.</li> </ul>"},{"location":"configuring/","title":"Configuring Spack Stacks","text":"<p>Stackinator generates the make files and spack configurations that build the spack environments that are packaged together in the spack stack. It can be thought of as equivalent to calling <code>cmake</code> or <code>configure</code>, performed using the <code>stack-config</code> CLI tool:</p> <pre><code># configure the build\n./bin/stack-config --build $BUILD_PATH --recipe $RECIPE_PATH --system $SYSTEM_CONFIG_PATH\n</code></pre> <p>The following flags are required:</p> <ul> <li><code>-b/--build</code>: the path where the build is to be performed.</li> <li><code>-r/--recipe</code>: the path with the recipe yaml files that describe the environment.</li> <li><code>-s/--system</code>: the path containing the system configuration for the target cluster.</li> </ul> <p>The following flags are optional:</p> <ul> <li><code>-c/--cache</code>: configure the build cache.</li> <li><code>-m/--mount</code>: override the mount point where the stack will be installed.</li> <li><code>--version</code>: print the stackinator version.</li> <li><code>-h/--help</code>: print help message.</li> </ul>"},{"location":"development/","title":"Development","text":"<p>This page is for developers and maintainers of Stackinator.</p>"},{"location":"development/#debug-environment","title":"Debug environment","text":"<p>Debugging stack builds can be challenging, because the build uses an environment with paths mounted and remounted using bwrap and different environment variables than the calling shell.</p> <p>A helper script that will open a new shell with the same environment as the stack build is generated in the build path. The script, <code>stack-debug.sh</code>, can be sourced to start the new bash shell:</p> <pre><code>user@hostname:/dev/shm/project-build &gt; source ./stack-debug.sh\nbuild-env &gt;&gt;&gt;\n</code></pre> <p>The new shell has <code>spack</code> in its path, and has the store path mounted at the environment's mount point. To finish debugging, exit the shell with <code>exit</code> or ctrl-d.</p>"},{"location":"installing/","title":"Installing Stacks","text":"<p>The installation path of the software stack is set when the stack is configured.</p> <p>The default location for a recipe is set in the <code>store</code> field of <code>config.yaml</code> in the recipe: config.yaml<pre><code>name: best-stack-ever\nstore: /user-environment\nspack:\ncommit: releases/v0.20\nrepo: https://github.com/spack/spack.git\n</code></pre></p> <p>The installation path can be overridden using the <code>--mount/-m</code> flag to <code>stack-config</code>. The software is built using rpaths hard-coded to the installation path, which simplifies dynamic linking  (<code>LD_LIBRARY_PATH</code> does not have to be set during run time).</p> <p>Alps</p> <p>For deployment on Alps, stacks should use the standard <code>/user-environment</code> mount point.</p> <p>Warning</p> <p>Environments built for one mount point should not be mounted at a different location. If a new mount point is desired, rebuild the stack for the new mount point.</p>"},{"location":"installing/#installing-the-software","title":"Installing the software","text":"<p>Running <code>make</code> to build the environment generates two versions of the software stack in the build path: <pre><code>build_path\n\u251c\u2500 store\n\u2514\u2500 store.squashfs\n</code></pre></p>"},{"location":"installing/#shared-file-system-installation","title":"Shared file system installation","text":"<p>The <code>store</code> sub-directory contains the full software stack installation tree.</p> <p>Note</p> <p>The \"simplest\" method for installing the software stack, that does not require installing additional tools to use the stack, is to copy the contents of <code>store</code> to the installation path.</p>"},{"location":"installing/#squashfs-installation","title":"SquashFS installation","text":"<p>The <code>store.squashfs</code> file is a compressed SquashFS image of the contents of the <code>store</code> path. This can be mounted at runtime using <code>squashfs-mount</code> or the Slurm plugin, or mounted by a system-administrator using <code>mount</code>, in order the to take advantage of the benefits of SquashFS over shared file systems.</p>"},{"location":"interfaces/","title":"Interfaces","text":"<p>Software stacks offer a choice of interfaces that can be presented to users.</p>"},{"location":"interfaces/#spack-upstream","title":"Spack Upstream","text":"<p>Every stack can be used as a Spack upstream for users of Spack on the system. This means that users can access all of the software packages and custom recipes provided by a software Spack directly in their Spack configuration.</p> <p>The installation contains a custom configuration scope in the <code>config</code> sub-directory, and additional information about custom Spack packages in the <code>repo</code> sub-directory. For example, the Spack configuration is in the following files when a stack has been installed at the default <code>/user-environment</code> mount point: <pre><code>/user-environment\n\u251c\u2500 config\n\u2502  \u251c\u2500 compilers.yaml\n\u2502  \u251c\u2500 repos.yaml\n\u2502  \u251c\u2500 packages.yaml\n\u2502  \u2514\u2500 upstreams.yaml\n\u2514\u2500 repo\n   \u251c\u2500 repo.yaml\n   \u2514\u2500 packages\n      \u2514\u2500 cray-mpich\n         \u2514\u2500 package.py\n</code></pre></p> <p>Notes on the configuration files:</p> <ul> <li><code>upstream.yaml</code>: Registers the spack packages installed in the Spack stack so that they will be found by the downstream user when searching for packages:     <pre><code>upstreams:\nsystem:\ninstall_tree: /user-environment\n</code></pre></li> <li><code>compilers.yaml</code>: includes all compilers that were installed in the <code>gcc:</code> and <code>llvm:</code> sections of the <code>compilers.yaml</code> recipe file. Note that the <code>bootstrap</code> compiler is not included.</li> <li><code>packages.yaml</code>: refers to the external packages that were used to configure the recipe: both the defaults in the cluster configuration, and any additional packages that were set in the recipe.</li> <li><code>repos.yaml</code>: points to the custom Spack repository:     <pre><code>repos:\n- /user-environment/repo\n</code></pre></li> </ul> <p>End users can use the Spack stack in their Spack installations in a variety of ways, including: <pre><code># set an environment variable\nexport SPACK_SYSTEM_CONFIG_PATH=/user-environment/config\n\n# pass on command line to Spack\nspack --config-scope /user-environment/config ...\n</code></pre></p> <p>See the Spack documentation for the diverse ways that custom configurations can be used.</p> <p>The <code>repo</code> path contains the custom <code>cray-mpich</code> package configuration. If the stack recipe provided additional custom packages, these will also be in sub-directories of <code>$install_path/repo/packages</code></p>"},{"location":"interfaces/#modules","title":"Modules","text":"<p>Module files can be provided as an optional interface, for users that and use-cases that prefer or require them.</p> <p>If modules are available, the generated module files are in the <code>modules</code> sub-directory of the installation path, and end users can make them available via <code>module use</code>:</p> <pre><code># make the modules available\nmodule use /user-environment/modules\n\n# list the available moduels\nmodule avail\n\n-------------------------- /user-environment/modules --------------------------\n   cmake/3.26.3    gcc/11.3.0       libtree/3.1.1               python/3.10.10\n   cray-mpich      hdf5/1.14.1-2    osu-micro-benchmarks/5.9    tree/2.1.0\n</code></pre>"},{"location":"interfaces/#environment-views","title":"Environment Views","text":"<p>File system views are an optional way to provide the software from an environment in a directory structure similar to <code>/usr/local</code>, based on Spack's filesystem views. See the recipe documentation for details on how to configure views.</p> <p>The views are created in the <code>env</code> path of the installation. As an example, given two views named <code>default</code> and <code>no-python</code> for an stack installed in the standard <code>/user-environment</code> location, then two directory trees named after the views are generated in <code>/user-environment/env</code>:</p> <pre><code>/user-environment\n\u2514\u2500 env\n   \u251c\u2500 default\n   \u2502  \u251c\u2500 bin\n   \u2502  \u251c\u2500 lib\n   \u2502  \u251c\u2500 ...\n   \u2502  \u2514\u2500 activate.sh\n   \u2514\u2500 no-python\n      \u251c\u2500 bin\n      \u251c\u2500 lib\n      \u251c\u2500 ...\n      \u2514\u2500 activate.sh\n</code></pre> <p>The <code>activate.sh</code> script in each view can be used to load the view by setting environment variables like <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, <code>CPATH</code> etc.</p> <pre><code>source /user-environment/env/no-python/activate.sh\n</code></pre> <p>Note</p> <p>Meta data about the environment views provided by a Spack stack is provided in the file <code>meta/env.json</code>.</p>"},{"location":"readme/","title":"Readme","text":"<p>The documentation for Stackinator is built from the markdown files in this path using MkDocs and MkDocs-material. You can view the latest documentation online at github.io</p> <p>To build a copy locally, first install <code>mkdocs-material</code>, e.g.: <pre><code>python3 -m venv docs-env\nsource docs-env/bin/activate\npip install mkdocs-material\n</code></pre></p> <p>Then in the root of this project, build the docs and view them with your favourite browser: <pre><code>mkdocs build\nfirefox site/index.html\n</code></pre></p>"},{"location":"recipes/","title":"Recipes","text":"<p>A recipe is a description of all of the compilers and software packages to be installed, along with configuration of modules and environment scripts the stack will provide to users. A recipe is comprised of the following yaml files in a directory:</p> <ul> <li><code>config.yaml</code>: common configuration for the stack.</li> <li><code>compilers.yaml</code>: the compilers provided by the stack.</li> <li><code>environments.yaml</code>: environments that contain all the software packages.</li> <li><code>modules.yaml</code>: optional module generation rules<ul> <li>follows the spec for spack module configuration</li> </ul> </li> <li><code>packages.yaml</code>: optional define external packages<ul> <li>follows the spec for spack package configuration</li> </ul> </li> <li><code>repo</code>: optional custom spack package definitions.</li> <li><code>extra</code>: optional additional meta data to copy to the meta data of the stack.</li> <li><code>post-install</code>: optional a script to run after Spack has been executed to build the stack.</li> </ul>"},{"location":"recipes/#configuration","title":"Configuration","text":"config.yaml<pre><code>name: prgenv-gnu\nstore: /user-environment\nspack:\nrepo: https://github.com/spack/spack.git\ncommit: releases/v0.20\nmodules: true\ndescription: \"HPC development tools for building MPI applications with the GNU compiler toolchain\"\n</code></pre> <ul> <li><code>name</code>: a plain text name for the environment</li> <li><code>store</code>: the location where the environment will be mounted.</li> <li><code>spack</code>: which spack repository to use for installation.</li> <li><code>modules</code>: optional enable/diasble module file generation (default <code>true</code>).</li> <li><code>description</code>: optional a string that describes the environment (default empty).</li> </ul>"},{"location":"recipes/#compilers","title":"Compilers","text":"<p>Take an example configuration: compilers.yaml<pre><code>bootstrap:\nspec: gcc@11\ngcc:\nspecs:\n- gcc@11\nllvm:\nrequires: gcc@11\nspecs:\n- nvhpc@21.7\n- llvm@14\n</code></pre></p> <p>The compilers are built in multiple stages:</p> <ol> <li>bootstrap: A bootstrap gcc compiler is built using the system compiler (currently gcc 4.7.5).<ul> <li><code>gcc:specs</code>: single spec of the form <code>gcc@version</code>.</li> <li>The selected version should have full support for the target architecture in order to build optimised gcc toolchains in step 2.</li> </ul> </li> <li>gcc: The bootstrap compiler is then used to build the gcc version(s) provided by the stack.<ul> <li><code>gcc:specs</code>: A list of at least one of the specs of the form <code>gcc@version</code>.</li> </ul> </li> <li>llvm: (optional) The nvhpc and/or llvm toolchains are built using one of the gcc toolchains installed in step 2.<ul> <li><code>llvm:specs</code>: a list of specs of the form <code>nvhpc@version</code> or <code>llvm@version</code>.</li> <li><code>llvm:requires</code>: the version of gcc from step 2 that is used to build the llvm compilers.</li> </ul> </li> </ol> <p>The first two steps are required, so that the simplest stack will provide at least one version of gcc compiled for the target architecture.</p> <p>Note</p> <p>Don't provide full specs, because the tool will insert \"opinionated\" specs for the target node type, for example:</p> <ul> <li><code>nvhpc@21.7</code> generates <code>nvhpc@21.7 ~mpi~blas~lapack</code></li> <li><code>llvm@14</code> generates <code>llvm@14 +clang targets=x86 ~gold ^ninja@kitware</code></li> <li><code>gcc@11</code> generates <code>gcc@11 build_type=Release +profiled +strip</code></li> </ul>"},{"location":"recipes/#environments","title":"Environments","text":"<p>The software packages to install using the compiler toolchains are configured as disjoint environments, each built with the same compiler, and configured with an optional implementation of MPI. These are specified in the <code>environments.yaml</code> file.</p> <p>For example, consider a workflow that has to build more multiple applications - some of which require Fortran+OpenACC and others that are CPU only C code that can be built with GCC. To provide a single Spack stack that meets the workflow's needs, we would create two environments, one for each of the <code>nvhpc</code> and <code>gcc</code> compiler toolchains:</p> environments.yaml high level overview<pre><code># A GCC-based programming environment\nprgenv-gnu:\ncompiler: # ... compiler toolchain\nmpi:      # ... mpi configuration\nunify:    # ... configure Spack concretizer\nspecs:    # ... list of packages to install\nvariants: # ... variants to apply to packages (e.g. +mpi)\npackages: # ... list of external packages to use\nviews:    # ... environment views to provide to users\n# An NVIDIA programming environment\nprgenv-nvgpu:\n# ... same structure as prgenv-gnu\n</code></pre> <p>In the following sections, we will explore each of the environment configuration fields in detail.</p>"},{"location":"recipes/#compilers_1","title":"Compilers","text":"<p>The <code>compiler</code> field describes a list compilers to use to build the software stack. Each compiler toolchain is specified using toolchain and spec</p> compile all packages with gcc@11.3<pre><code>  compiler\n- toolchain: gcc\nspec: gcc@11.3\n</code></pre> <p>Sometimes two compiler toolchains are required, for example when using the <code>nvhpc</code> compilers, there are often dependencies that can't be built using the NVIDIA, or are better being built with GCC (for example <code>cmake</code>, <code>perl</code> and <code>netcdf-c</code>). The example below uses the <code>nvhpc</code> compilers with gcc@11.3.</p> compile all packages with gcc@11.3<pre><code>  compiler\n- toolchain: llvm\nspec: nvhpc@22.7\n- toolchain: gcc\nspec: gcc@11.3\n</code></pre> <p>Note</p> <p>If more than one version of gcc has been installed, use the same version that was used to install <code>nvhpc</code>.</p> <p>Warning</p> <p>As a rule, use a single compiler wherever possible - keep it simple!</p> <p>We don't test or support using two versions of gcc in the same toolchain.</p>"},{"location":"recipes/#mpi","title":"MPI","text":"<p>Stackinator can configure cray-mpich (CUDA, ROCM, or non-GPU aware) on a per-environment basis, by setting the <code>mpi</code> field in an environment.</p> <p>Note</p> <p>Future versions of Stackinator will support OpenMPI, MPICH and MVAPICH when (and if) they develop robust support for HPE SlingShot 11 interconnect.</p> <p>If the <code>mpi</code> field is not set, or is set to <code>null</code>, MPI will not be configured in an environment: environments.yaml: no MPI<pre><code>serial-env:\nmpi: null\n# ...\n</code></pre></p> <p>To configure MPI without GPU support, set the <code>spec</code> field with an optional version: environments.yaml: MPI without GPU support<pre><code>host-env:\nmpi:\nspec: cray-mpich@8.1.23\n# ...\n</code></pre></p> <p>GPU-aware MPI can be configured by setting the optional <code>gpu</code> field to specify whether to support <code>cuda</code> or <code>rocm</code> GPUs: environments.yaml: GPU aware MPI<pre><code>cuda-env:\nmpi:\nspec: cray-mpich\ngpu: cuda\n# ...\nrocm-env:\nmpi:\nspec: cray-mpich\ngpu: rocm\n# ...\n</code></pre></p> <p>As new versions of cray-mpich are released with CPE, they are added to Stackinator. The following versions of cray-mpich are currently provided:</p> cray-mpich CPE notes 8.1.25 23.03 released 2023-02-26 default 8.1.24 23.02 released 2023-01-19 8.1.23 22.12 released 2022-11-29 8.1.21.1 22.11 released 2022-10-25 8.1.18.4 22.08 released 2022-07-21 <p>Alps</p> <p>All versions of cray-mpich in the table have been validated on Alps vClusters with Slingshot 11 and libfabric 1.15.2.</p> <p>Note</p> <p>The <code>cray-mpich</code> spec is added to the list of package specs automatically, and all packages that use the virtual dependency <code>+mpi</code> will use this <code>cray-mpich</code>.</p>"},{"location":"recipes/#specs","title":"Specs","text":"<p>The list of software packages to install is configured in the <code>spec:</code> field of an environment. The specs follow the standard Spack practice.</p> <p>The <code>unify:</code> field controls the Spack concretiser, and can be set to three values <code>true</code>, <code>false</code> or <code>when_possible</code>. The </p> <pre><code>cuda-env:\nspecs:\n- cmake\n- hdf5\n- python@3.10\nunify: true\n</code></pre> <p>To install more than one version of the same package, or to concretise some more challenging combinations of packages, you might have to relax the concretiser to <code>when_possible</code> or <code>false</code>. For example, this environment provides <code>hdf5</code> both with and without MPI support:</p> <pre><code>cuda-env:\nspecs:\n- cmake\n- hdf5~mpi\n- hdf5+mpi\n- python@3.10\nunify: when_possible\n</code></pre> <p>Note</p> <p>Use <code>unify:true</code> when possible, then <code>unify:when_possible</code>, and finally <code>unify:false</code>.</p> <p>Warning</p> <p>Don't provide a spec for MPI or Compilers, which are configured in the <code>mpi:</code> and <code>compilers</code> fields respecively.</p> <p>Warning</p> <p>Stackinator does not support \"spec matrices\", and likely won't, because they use multiple compiler toolchains in a manner that is contrary to the Stackinator \"keep it simple\" principle.</p>"},{"location":"recipes/#packages","title":"Packages","text":"<p>To specify external packages that should be used instead of building them, use the <code>packages</code> field. For example, if the <code>perl</code>, <code>python@3</code> and <code>git</code> packages are build dependencies of an environment and the versions that are available in the base CrayOS installation are sufficient, the following spec would be specified:</p> environments.yaml: specifying external packages<pre><code>my-env:\npackages:\n- perl\n- git\n</code></pre> <p>Note</p> <p>If a package is not found, it will be built by Spack.</p> <p>Note</p> <p>External packages specified in this manner will only be used when concretising this environment, and will not affect downstream users.</p> expand if you are curious how Stackinator configures Spack for packages <p>The following Spack call is used to generate <code>packages.yaml</code> in the Spack environment that Stackinator generates in the build path to concretise and build the packages in the example above:</p> Makefile target for external packages in an environment<pre><code>packages.yaml:\n    spack external find --not-buildable --scope=user perl git\n</code></pre>"},{"location":"recipes/#variants","title":"Variants","text":"<p>To specify variants that should be applied to all package specs in the environment by default (unless overridden explicitly in a package spec), use the <code>variants</code> field. For example, to concretise all specs in an environment that support MPI or CUDA and target A100 GPUs, the following <code>variants</code> could be set:</p> environments.yaml: variants for MPI and CUDA on A100<pre><code>cuda-env:\nvariants:\n- +mpi\n- +cuda\n- cuda_arch=80\n</code></pre> expand if you are curious how Stackinator configures Spack for variants <p>The above will add the following to the generated <code>spack.yaml</code> file used internally by Spack.</p> spack.yaml: packages spec generated for variants<pre><code>spack:\npackages:\nall:\nvariants:\n- +mpi\n- +cuda\n- cuda_arch=80\n</code></pre>"},{"location":"recipes/#views","title":"Views","text":"<p>File system views are an optional way to provide the software from an environment in a directory structure similar to <code>/usr/local</code>, based on Spack's filesystem views.</p> <p>Each environment can provide more than one view, and the structure of the YAML is the same as used by the version of Spack used to build the Spack stack. For example, the <code>views</code> description:</p> <pre><code>cuda-env:\nviews:\ndefault:\nno-python:\nexclude:\n- 'python'\n</code></pre> <p>will configure two views:</p> <ul> <li><code>default</code>: a view of all the software in the environment using the default settings of Spack.</li> <li><code>no-python</code>: everything in the default view, except any versions of <code>python</code>.</li> </ul> <p>See the interfaces documentation for more information about how the environment views are provided to users of a stack.</p>"},{"location":"recipes/#modules","title":"Modules","text":"<p>Modules are generated for the installed compilers and packages by spack. The default module generation rules set by the version of spack specified in <code>config.yaml</code> will be used if no <code>modules.yaml</code> file is provided.</p> <p>To set rules for module generation, provide a <code>modules.yaml</code> file as per the spack documentation.</p> <p>To disable module generation, set the field <code>config:modules:False</code> in <code>config.yaml</code>.</p>"},{"location":"recipes/#custom-spack-packages","title":"Custom Spack Packages","text":"<p>An optional package repository can be added to a recipe to provide new or customized Spack packages in addition to Spack's <code>builtin</code> package repository, if a <code>repo</code> path is provided in the recipe.</p> <p>For example, the following <code>repo</code> path will add custom package definitions for the <code>hdf5</code> and <code>nvhpc</code> packages:</p> <pre><code>repo\n\u2514\u2500 packages\n   \u251c\u2500 hdf5\n   \u2502  \u2514\u2500 package.py\n   \u2514\u2500 nvhpc\n      \u2514\u2500 package.py\n</code></pre> <p>Stackinator internally provides its own package repository with a custom package for <code>cray-mpich</code> package, which it puts in the <code>alps</code> namespace. The <code>alps</code> repository is installed alongside the packages, and is automatically available to all Spack users that use the Spack stack as an upstream.</p> <p>Warning</p> <p>Unlike Spack package repositories, any <code>repos.yaml</code> file in the <code>repo</code> path will be ignored and a warning will be issued. This is because the provided packages are added to the <code>alps</code> namespace.</p>"},{"location":"recipes/#post-install-configuration","title":"Post install configuration","text":"<p>If a script <code>post-install</code> is provided in the recipe, it will be run during the build process: after the stack has been built, and just before the final squashfs image is generated. Post install scripts can be used to modify or extend an environment with operations that can't be performed in Spack, for example:</p> <ul> <li>configure a license file;</li> <li>install additional software outside of Spack;</li> <li>generate activation scripts.</li> </ul> <p>The following steps are effectively run, where we assume that the recipe is in <code>$recipe</code> and the mount point is the default <code>/user-environment</code>:</p> <pre><code># copy the \ncp \"$recipe\"/post-install /user-environment\nchmod +x /user-environment/post-install\n\n# apply Jinja templates\njinja -d env.json /user-environment/post-install &gt; /user-environment/post-install\n\n# execute the script from inside the mount point\ncd /user-environment\n/user-environment/post-install\n</code></pre> <p>The post-install script is templated using Jinja, with the following variables available for use in a script:</p> Variable Description <code>env.mount</code> The mount point of the image - default <code>/user-environment</code> <code>env.config</code> The installation tree of the Spack installation that was built in previous steps <code>env.build</code> The build path <code>env.spack</code> The location of Spack used to build the software stack (only available during installation) <p>The use of Jinja templates is demonstrated in the following example of a bash script that generates an activation script that adds the installation path of GROMACS to the system PATH:</p> post-install script that generates a simple activation script.<pre><code>#!/bin/bash\ngmx_path=$(spack -C {{ env.config }} location -i gromacs)/bin\necho \"export PATH=$gmx_path:$PATH\" &gt;&gt; {{ env.mount }}/activate.sh\n</code></pre> <p>Note</p> <p>The copy of Spack used to build the stack is available in the environment in which <code>post-install</code> runs, and can be called directly.</p> <p>Note</p> <p>The script does not have to be bash - it can be in any scripting language, such as Python or Perl, that is available on the target system.</p>"},{"location":"recipes/#meta-data","title":"Meta-Data","text":"<p>Stackinator generates meta-data about the stack to the <code>extra</code> path of the installation path. A recipe can install arbitrary meta data by providing a <code>extra</code> path, the contents of which will be copied to the <code>meta/extra</code> path in the installation path.</p> <p>Alps</p> <p>This is used to provide additional information required by ReFrame as part of the CI/CD pipeline for software stacks on Alps, defined in the GitHub eth-cscs/alps-spack-stacks repository.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>TODO</p> <p>write a tutorial that explains building an image step by step.</p> <p>A spack stack with everything needed to develop Arbor on for the A100 nodes on Hohgant.</p> <p>This guide walks us through the process of configuring a spack stack, building and using it.</p> <p>Arbor is a C++ library, with optional support for CUDA, MPI and Python. An Arbor developer would ideally have an environment that provides everything needed to build Arbor with these options enabled.</p> <p>The full list of all of the Spack packages needed to build a full-featured CUDA version is:</p> <ul> <li>MPI: <code>cray-mpich-binary</code></li> <li>compiler: <code>gcc@11</code></li> <li>Python: <code>python@3.10</code></li> <li>CUDA: <code>cuda@11.8</code></li> <li><code>cmake</code></li> <li><code>fmt</code></li> <li><code>pugixml</code></li> <li><code>nlohmann-json</code></li> <li><code>random123</code></li> <li><code>py-mpi4py</code></li> <li><code>py-numpy</code></li> <li><code>py-pybind11</code></li> <li><code>py-sphinx</code></li> <li><code>py-svgwrite</code></li> </ul> <p>For the compiler, we choose <code>gcc@11</code>, which is compatible with cuda@11.8.</p>"}]}